---
layout: single
title: "Machine Learning Specialization, Lecture 1"
permalink: /deeplearning-ai/machine-learning/1/
mathjax: true
---

[View Lecture 1 PDF](C1_W1.pdf){:target="_blank"}

## Linear Regression with One Variable

- Regression vs Classification
  - Regression: Predict a continuous value (e.g., house price)
  - Classification: Predict a discrete label (e.g., spam or not spam)
- Linear regression: Predict a continuous value using a linear function of the input features.

- Cost function: Measures how well the model fits the data.
  - Mean Squared Error (MSE): Average of the squared differences between predicted and actual values.
- Gradient descent: Optimization algorithm to minimize the cost function.
  - Update the model parameters iteratively to reduce the cost.

- Cost function (mean squared error):

$$
J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2
$$

(Reason of have a factor of 1/2 in the cost function: it simplifies the derivative calculation for gradient descent.)

The cost function is a measure of how well the model fits the data. The goal of linear regression is to minimize this cost function.
- The cost function is a convex function, meaning it has a single global minimum.
(because squared error always results positive values)
- The gradient descent algorithm iteratively updates the model parameters to minimize the cost function.